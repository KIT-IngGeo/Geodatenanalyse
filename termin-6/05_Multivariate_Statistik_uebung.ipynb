{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geodatenanalyse 1\n",
    "\n",
    "## Übung 5: Multivariate Statistik - Übung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Übung wollen wir uns mit der Analyse und Visualisierung von mehrdimensionalen Daten befassen. Dazu lesen wir zunächst den vollständigen Datensatz (Data_GW_KA.csv) mit den Grundwasserparametern aus Karlsruhe in Python ein. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # pandas package einladen und mit der Abkürzung 'pd' versehen\n",
    "data = pd.read_csv('Data_GW_KA.csv', sep=';', encoding='cp1252') # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In 5 Schritten zur Hauptkomponentenanalyse\n",
    "\n",
    "#### 1. Standardisierung\n",
    "\n",
    "Da wir in der letzten Übung gesehen haben, dass die Varianzen und Kovarianzen der Parameter in dem Datensatz unterschiedliche Größenordnungen haben, sollten die Werte sämtlicher Parameter vor der weiteren Analyse auf eine Standard-Normalvereilung transformiert werden. \n",
    "\n",
    "Dazu gibt es in dem Python Package `sklearn` die Funktion `sklearn.preprocessing.StandardScaler().fit_transform()`. Ladet diese Funktion zunächst in Euren Code ein. \n",
    "\n",
    "Für die Transformierung erzeugt Ihr als nächsts eine neue Variable (output) mit Hilfe dieser Funktion. Die erste Klammer der Funktion bleit bei der Anwendung hier leer, in die hintere fügt Ihr den Namen des zu transformierenden Datensatzes ein (input). \n",
    "\n",
    "Inspiziert anschließend den standardisierten Datensatz mit Hilfe des Befehls `print()`, und überprüft ob die Daten wie gewünscht vorwiegend zwischen -1 und +1 liegen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] \n",
    "# benötigtes Packages/benötigte Funktion einladen\n",
    "\n",
    "# output = Funktion(input)\n",
    "\n",
    "# print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Eigenwerte und Eigenvektoren bestimmen\n",
    "\n",
    "Berechnet für die Bestimmung der Eigenwerte zuerst die Kovarianzmatrix der standardisierten Daten mit Hilfe von `numpy` und der Funktion `numpy.cov(data.T)`. Wichtig ist hier das `.T` hinter dem Datensatz, das die transponierte Matrix von dem Datensats bildet, das vereinfacht die weitere Handhabung der Matrizen. \n",
    "\n",
    "Anschließend könnt Ihr mit der Funktion `numpy.linalg.eig()` die Eigenwerte und Eigenvektoren der Kovarianzmatrix berechnen. Dafür müsst Ihr zwei Outputs definieren, und als Input die zuvor berechnete Kovarianzmatrix nehmen.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2]\n",
    "# benötigtes Packages/benötigte Funktion einladen\n",
    "\n",
    "# Kovarianzmatrix berechnen\n",
    "\n",
    "# Eigenwerte und Eigenvektoren berechnen \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Hauptkomponenten bestimmen\n",
    "\n",
    "Das Ziel der Hauptkomponentenanalyse ist es die Dimensionen des Datensatzes zu reduzieren, z.B. auf zwei für eine 2D-Visualisierung. Die Richtung dieser Achsen im Parameterraum entspricht den Eigenvektoren (mit Einheitslänge 1). Damit möglichst viel Information in Form der Varianz in den zwei Dimensionen erhalten bleibt, suchen wir nun jene Eigenvektoren mit den größter Eigenwerten. \n",
    "\n",
    "Definiert dafür einen Parameter vom Typ `list` ohne Inhalt als `name = [[]]*n` (vgl. Termin 1, Block 2), mit `n` als Dimension entsprechend der Anzahl der Eigenwerte. Darin wollen wir in jeder Zeile jeweils die gepaarten Eigenwerte und Eigenvektoren abbilden.  \n",
    "\n",
    "Füllt dafür die Zeilen der Liste über eine `for` Schleife (mit der Länge entsprechend der Anzahl der Eigenwerte) mit den Absolutwerten der jeweiligen Eigenwerten und den Werten der Eigenvektoren. Die Absolutwerte eines Arrays könnt Ihr mit Hilfe von `numpy.abs(array)` berechnen. \n",
    "\n",
    "Achtung: in der Schleife erfolgt die Indizierung bei den Eigenwerten zeilenweise (also mit einem einfachem Index [i]), die Indizierung der Eigenvektoren spaltenweise (also [:,i]), da es sich bei diesem um eine 15x15 Matrix handelt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3]\n",
    "# neuen, leeren Parameter erzeugen\n",
    "\n",
    "# for-Schleife definieren \n",
    "# for Schleife füllen: Zeile[i] = [(Absolutwert_Eigenwert, Werte_Eigenvektoren)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun müssen wir noch die Zeilen mit den größten Eigenwerten identifizieren. Sortiert dafür die Liste mit den Eigenwerten- und Eigenvektoren-Paaren in absteigender Reihenfolge. \n",
    "\n",
    "Bei Listen könnt ihr den Befehl `Liste.sort()` benutzen um Euch die Werte in aufsteigende Reihenfolgen ausgeben zu lassen. Dabei wird der erste Wert jeder Zeile berücksichtigt (also hier der Eigenwert). \n",
    "\n",
    "Danach könnt Ihr mit `Liste.reverse()` davon die umgekehrte Reihenfolge bilden, sodass der größte Eigenwert oben steht. \n",
    "\n",
    "Überprüft dies mit Hilfe des `print()`-Befehls.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4]\n",
    "# Wertepaare aufsteigend sortieren \n",
    "\n",
    "# Reihenfolge umkehren\n",
    "\n",
    "# sortierte Wertepaare anzeigen lassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die zwei obersten Zeileneinträge entsprechen den gesuchten Hauptkomponenten entlang der größten Varianzen. Speichert diese in separaten Variablen (ebenfalls vom Typ `list`) ab `name1=name2[index]`. \n",
    "Benutzt den korrekten Index für Zeilen! \n",
    "\n",
    "Lasst Euch die beiden Variablen nochmal zur Kontrolle anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5]\n",
    "# Hauptkomponenten 1 und 2 basierend auf Wertepaaren definieren\n",
    "\n",
    "# Hauptkomponenten anzeigen lassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Projektionsmatrix konstruieren\n",
    "\n",
    "Mit Hilfe der Projektionsmatrix wollen wir nun die ursprünglichen Daten auf die zwei eben identifizierten Achsen der Hauptkomponenten transformieren. Um 15 Parameter auf 2 Dimensionen zu reduzieren brauchen wir also eine (15 x 2) Matrix, deren Spalten den beiden Eigenvektoren mit den größten Eigenwerten entsprechen. \n",
    "\n",
    "Diese Eigenvektoren habt Ihr bereits im letzten Schritt (zusammen mit den Eigenwerten) als `list` gespeichtert. Fügt die beiden obersten Vektoren nun mit Hilfe von `numpy.stack((Vektor1, Vektor2))` und dem zusätzlichen Argument `axis=-1` für eine horiziontale Ausrichtung zu einer Matrix `W` zusammen. \n",
    "\n",
    "Als Erinnerung: Die Indizierung bei Listen funktioniert nach dem Prinzip `\"list[Zeile][Spalte]\"`. Schaut nochmal im letzten Ausgabefenster oben nach, in welcher Zelle der Hauptkomponenten sich die Eigenvektoren befinden.  \n",
    "\n",
    "Lasst Euch anschließend \"W\" anzeigen, um die Dimensionen zu überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "# W definieren\n",
    "\n",
    "# W anzeigen lassen \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Projektion auf neue Ebenen und Visualisierung\n",
    "\n",
    "Jetzt könnt Ihr über die Gleichung `Y = data x W ` die Transformation durchführen. Matrixmultiplikation in Python kann mit `matrix_1.dot(matrix_2)` durchgeführt werden. \n",
    "\n",
    "Die Output-Matrix Y sollte dann 39 x 2 (Datenpunkte x Hauptkomponenten) Felder haben. Diese Anzahl könnt Ihr überprüfen indem Ihr Euch die Größe der Matrix über `numpy.size(matrix)` anzeigen lasst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7]\n",
    "# Matrixmultiplikation\n",
    "\n",
    "# Größe der Matrix anzeigen lassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese transformierte Matrix können wir nun mit Hilfe des Packages `pandas` (mehr dazu nächste Woche) in den Datentyp \"DataFrame\" umwandeln, das erleicntert das weitere Arbeiten damit. Gleichzeitig können wir in dem DataFrame eine Spalte mit dem Parameter \"Flaechennutzung\" ergänzen. \n",
    "\n",
    "Ergänzt nun in dem Skript unten mit Hilfe von `matplotlib` einen Scatterplot mit den beiden Hauptkomponenten als Achsen, und der Flaechennutzung als Farbe der Punkte. Was lässt sich anhand dieser Abbildung über die Parameterwerte in Bezug auf die Flaechennutzung sagen?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [8]\n",
    "# aus ndarray einen DataFrame erzeugen \n",
    "principalDf = pd.DataFrame(data = Y , columns = ['principal component 1', 'principal component 2'])\n",
    "# eine dritte Spalte mit den Werten der Flächennutzung ergänzen\n",
    "finalDf = pd.concat([principalDf,pd.DataFrame(data,columns = ['Flaechennutzung'])], axis = 1) \n",
    "\n",
    "# hier Abbildung mit matplotlib erzeugen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hauptkomponentenanalyse mit sklearn\n",
    "\n",
    "Das Python Package `sklearn` enthält viele nützliche Funtkionen für statistische Analysen und maschinelles Lernen. Darunter auch eine Funktion für Hauptkomponentenanalyse `sklearn.decomposition.PCA()`. \n",
    "\n",
    "Definiert (nach dem Einladen des Packages) für die Anwendung zuerst ein Objekt mit der genauen Methode (d.h. ein Output mit dem Namen \"model\") mit Hilfe der Funktion `sklearn.decomposition.PCA()`. Definiert als Input wie viele Hauptkomponenten (\"n_components=2\") Ihr ausgegeben haben möchtet. \n",
    "\n",
    "Im nächsten Schritt könnt Ihr den reduzierten Datensatz berechnen, indem Ihr auf diese Methode das Attribut `.fit_transform()` anwendet, mit dem ursprünglichen Datensatz als Input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9]\n",
    "# Package einladen\n",
    "\n",
    "# Model-Objekt erzeugen\n",
    "\n",
    "# Fit-Funktion auf Model-Objekt anwenden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Bewertung der Aussagekraft einer Hauptkomponentenanalyse ist es wichtig zu wissen, wie viel der ursprünglich Varianz (und damit der Informationen) in dem neuen transformierten Datensatz enthalten ist. Für die einzelnen Komponenten könnt Ihr das ausrechnen, indem Ihr das Attribut `.explained_variance_ratio_` auf Euer PCA Objekt anwendet. \n",
    "\n",
    "Berechnet außerdem die Summe der Varianzen (z.B. mit der Funktion `sum()` ), und lasst Euch schließlich die Werte beider Variablen anzeigen. \n",
    "\n",
    "Wie würdet Ihr die Werte einordnen und die Aussagekraft bewerten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10]\n",
    "# einzelne Varianzen berechnen \n",
    "\n",
    "# Summe der Varianzen bilden\n",
    "\n",
    "# beide Variablen ausgeben lassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun stellt auch die Ergebnisse der PCA mit `sklearn` analog zu oben graphisch dar, und vergleicht die beiden Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [11]\n",
    "principalDf2 = pd.DataFrame(data = data_reduced, columns = ['principal component 1', 'principal component 2'])\n",
    "finalDf2 = pd.concat([principalDf2,pd.DataFrame(data,columns = ['Flaechennutzung'])], axis = 1) \n",
    "\n",
    "# hier Abbildung erzeugen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Ihr alles richtig gemacht habt, sollten die beiden Abbildungen das gleiche Bild zeigen (u.U. gespiegelt oder rotiert, also mit einem Vorzeichenwechsel auf einer oder beiden Achsen). \n",
    "\n",
    "## Ende\n",
    "\n",
    "### Referenzen: \n",
    "\n",
    "Koch et al. (2020), Groundwater fauna in an urban area: natural or affected? https://hess.copernicus.org/preprints/hess-2020-151/hess-2020-151.pdf\n",
    "\n",
    "Lever et al. (2017) Principal component analysis, Nature Methods 14(7), 641-642\n",
    "\n",
    "https://towardsdatascience.com/a-complete-guide-to-principal-component-analysis-pca-in-machine-learning-664f34fc3e5a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
