{
 "cells": [
  {
   "source": [
    "# Geodatenanalyse 1\n",
    "\n",
    "## Tag 1 / Block 2 / Übung 8: Grundlagen der Sensitivitätsanalyse"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Contribution-to-Variance\n",
    "\n",
    "\n",
    "\n",
    "Als Anwendungsbeispiel für eine Sensitivitätsanalyse werden wir das Model und die Unsicherheitsanalyse aus der letzten Übung verwenden. Kopiert daher zuerst das Skript mit der MC Simulation in dieses Notebook, damit ihr alle Input- und Outputwerte als Variablen zur Verfügung habt. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Monte-Carlo Simulation zur Bestimmung der Abbaurate von O-Xylol\n"
   ]
  },
  {
   "source": [
    "Für die Contribution-to-Variance Analyse und die Berechnung von Kovarianzen und Korrelationen ist es praktisch, alle benötigten Werte in einem DataFrame zusammenzufassen, um sicherzustellen, dass die Dimensionen, Ausrichtung von Spalten usw. stimmen. \n",
    "\n",
    "Hier nochmal ein Hinweis zum pandas Syntax für das Zusammenfassen von Arrays als Spalten: data = pd.DataFrame({'column_name': column_value, ...})\n",
    "\n",
    "Berechnet anschließend die Kovarianzen (`data.cov()`) und Korrelationen nach Pearson (`data.corr()`) für den DataFrame. Welches Maß macht in unserem Fall hier mehr Sinn für eine Betrachtung der Sensitivitäten? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] hier Code eingeben"
   ]
  },
  {
   "source": [
    "Stellt nun die Ergebnisse für die Sensitivitätsanalyse, sowohl für den kf-Wert als auch die Abbaurate (`subplot`), graphisch in einem Tornadoplot dar. Am einfachsten geht das über ein horizontales Blaknediagramm mit `matplotlib.pyplot.barh()`. Überlegt Euch auch genau welche Werte aus der Korrelations-, bzw- Kovarianzmatrix, enthalten sein sollten, und verwendet `labels` für die Achsenbeschriftung. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] hier Code eingeben\n"
   ]
  },
  {
   "source": [
    "### Resampling \n",
    "\n",
    "In Python lässt sich ein Resampling-Datensatz über die Funktion `np.random.choice()` erzeugen. Als Inputs werden hier der ursprüngliche Array, die gewünschte Größe (\"size\") des Resamplingdatensatzes, sowie eine Angabe zu `replace` benötigt. \n",
    "\n",
    "`replace=True` entspricht einem Bootstrap-Resampling, `replace=False` einem Jackknife-Resampling. Entsprechend muss bei einem Jackknife-Resampling `size` kleiner als die Größe der Ausgangs-Arrays gesetzt werden.  \n",
    "\n",
    "Erzeugt nun ein Bootstrap-Resampling des MC Inputdatensatzes für eine erneute Berechnung von kf und lambda. Geht dazu folgendermaßen vor: \n",
    "\n",
    "1. Originaldaten aus dem ersten Skriptfenster in einen gemeinsam DataFrame packen (s. Pandas Syntax oben)\n",
    "\n",
    "2. Einen neuen leeren Datensatz für das Resampling erzeugen. Dabei müssen Anzahl und Bezeichnung des Index und der Spalten identisch zum eben definierten Ausgangsdatensatz sein. Das geht über den pandas Befehl `pd.DataFrame(index=data.index, columns=input_data.columns)`\n",
    "\n",
    "3. Spaltenweise mit einer for-Schleife und `np.random.choice()` die Inputdaten resamplen  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] hier Code eingeben"
   ]
  },
  {
   "source": [
    "Basierend auf dem Resamlingdatensatz könnt Ihr nun erneut kf und lambda berechnen. Am besten in zwei neue Arrays (ohne die ursprünglichen zu überschreiben), damit Ihr die ursprünglichen Outputs mit denen des Resamlings vergleichen könnt. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] hier Code eingeben"
   ]
  },
  {
   "source": [
    "Anschließend können nun die Resampling-Outputs mit den Resampling-Inputs vereint werden, um eine neue Korrelationsmatrix zu berechnen. Vergleicht dann die beiden Korrelationsmatrizen (z.B. graphisch). \n",
    "\n",
    "Hinweis: Da hier nun verschiedenen Python-Datentypen vorliegen, ist es geschickt zuerst aus den neuen Arrays mit kf und lambda einen neuen DataFrame erstellen (s. Syntax oben), und diesen dann über `pd.concat([data1, data2], axis=1)` spaltenmweise mit den Resampling-Inputs vereinen. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "# Input und Output Datensätze vereinen \n",
    "\n",
    "\n",
    "# Korrelation berechnen\n",
    "\n",
    "\n",
    "# Korrelationen visualisieren \n"
   ]
  },
  {
   "source": [
    "Damit ist der Workflow für genau eine weitere Auswertung der Korrelation, bzw. Sensitivität abgeschlossen. Wenn wir nun ein for-Schleife mit bspw. 500 Wiederholungen um diesen Workflow legen würden, könnten wir die Unsicherheit in der Korrelation, und damit der Sensitivität, analysieren. \n",
    "\n",
    "Wer möchte, kann es ja mal probieren ;)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Schleife für resampling "
   ]
  },
  {
   "source": [
    "## Ende\n",
    "\n",
    "### Referenzen: \n",
    "\n",
    "Würth et al. (2021): Quantifying biodegradation rate constants of o-xylene by combining compound-specific isotope analysis and groundwater dating. Journal of Contaminant Hydrology, 238, 103757"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}