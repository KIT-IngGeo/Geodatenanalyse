{
 "cells": [
  {
   "source": [
    "# Geodatenanalyse 1\n",
    "\n",
    "## Tag 2 / Block 2 / Übung 5: Multivariate Statistik"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In dieser Übung wollen wir uns mit der Analyse und Visualisierung von mehrdimensionalen Daten befassen. Lest dazu zunächst den vollständigeb Datensatz mit den Grundwasserparametern aus Karlsruhe in Python ein. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# [1] hier Code eingeben\n",
    "import pandas as pd\n",
    "data = pd.read_csv('Data_GW_KA.csv', sep=';', encoding='cp1252')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### In 5 Schritten zur Hauptkomponentenanalyse\n",
    "\n",
    "#### 1. Standardisierung\n",
    "\n",
    "Da wir in der letzten Übung gesehen haben, dass die Varianzen und Kovarianzen der Parameter in dem Datensatz sehr unterschiedlich sind, sollten die Daten aller Parameter vor der weiteren Analyse an die Standard-Normalvereilung angepasst werden. Dazu gibt es in dem Python Package `sklearn` die Funktion `sklearn.preprocessing.StandardScaler().fit_transform()`. Die erste Klammer bleibt dabei leer, fügt den Namen Eures Datensatzes in der zweiten Klammer ein. \n",
    "\n",
    "Inspiziert anschließend den standardisierten Datendsatz und überprüft ob die Daten wie gewünscht vorwiegend zwischen -1 und +1 liegen. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] hier Code eingeben"
   ]
  },
  {
   "source": [
    "#### 2. Eigenwerte und Eigenvektoren bestimmen\n",
    "\n",
    "Berechnet für die Bestimmung der Eigenwerte zuerst die Kovarianzmatrix mit Hilfe von `numpy` und der Funktion `numpy.cov(data.T)`. Wichtig ist hier das `.T` hinter dem Datensatz, das die transponierte Matrix von dem Datensats bildet, das vereinfacht die weitere Handhabung der Matrizen. Anschließend könnt Ihr mit der Funktion `numpy.linalg.eig()` die Eigenwerte und Eigenvektoren der Kovarianzmatrix berechnen. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] hier Code eingeben"
   ]
  },
  {
   "source": [
    "Um die Zwischenergebnisse aus der Berechnung der Eigenwerte zu verifizieren, könnt Ihr an dieser Stelle überprüfen, ob die Summe der quadrierten Werte in den einzelen Eigenvektoren 1 ergibt. \n",
    "\n",
    "Definiert dafür zuerst einen leeren Vektor in den benötigten Dimensionen `[15, 1]` über die Funktion `numpy.empty()`. Benutzt dann eine `for` Schleife um für jede Spalte `[:,i]` des Eigenvektors die Summe der quadrierten Zahlenwerte zu erhalten. Diese sollte dann jeweils 1 ergeben. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] hier Code eingeben\n"
   ]
  },
  {
   "source": [
    "#### 3. Hauptkomponenten bestimmen\n",
    "\n",
    "Das Ziel der Hauptkomponentenanalyse ist es die Dimensionen des Datensatzes zu reduzieren, z.B. auf zwei für eine 2D-Visualisierung. Die Richtung dieser Achsen im Parameterraum entspricht den Eigenvektoren (mit Einheitslänge 1). Damit möglichst viel Information in Form der Varianz, in den zwei Dimensionen erhalten bleibt, suchen wir nun jene Eigenvektoren mit den größter Eigenwerten. \n",
    "\n",
    "Definiert dafür einen Parameter vom Typ `list` ohne Inhalt als `[[]]*n`, mit *n* als Dimension entsprechend der Anzahl der Eigenwerte. Dann füllt diese Liste über eine `for` Schleife mit den Absolutwerten der jeweiligen Eigenwerten und den Eigenvektoren (spaltenweise). \n",
    "\n",
    "Schlussendlich sortiert diese Liste noch in absteigender Reihenfolge: `data.sort()` gibt Euch die aufsteigende Reihenfolge, `data.reverse()` davon die umgekehrte Reihenfolge. Die zwei obersten Einträge entsprechen dann den gesuchten Hauptkomponenten entlang den größten Varianzen. \n",
    "\n",
    "Speichert diese zwei obersten Einträge in separaten Variablen vom Typ `list` ab.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] hier Code eingeben"
   ]
  },
  {
   "source": [
    "#### 4. Projektionsmatrix konstruieren\n",
    "\n",
    "Mit Hilfe der Projektionsmatrix wollen wir die ursprünglichen Daten auf die zwei neuen Achsen der Hauptkomponenten transformieren. Um die 15 Parameter auf 2 Dimensionen zu reduzieren brauchen wir also eine (15 x 2) Matrix, deren Spalten den beiden Eigenvektoren mit den größten Eigenwerten entsprechen. \n",
    "\n",
    "Diese Eigenvektoren habt Ihr bereits im letzten Schritt (zusammen mit den Eigenwerten) als `list` gespeichtert. Fügt diese Vektoren nun mit Hilfe von `numpy.stack()` und dem Argument `axis=-1` für eine horiziontale Ausrichtung zu einer Matrix `W` zusammen. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6] hier Code eingeben"
   ]
  },
  {
   "source": [
    "#### 5. Projektion auf neue Ebenen und Visualisierung\n",
    "\n",
    "Jetzt könnt Ihr über die Gleichung `Y = data x W ` die Transformation durchführen. Matrixmultiplikation in Python kann mit `matrix_1.dot(matrix_2)` durchgeführt werden. Die Output-Matrix Y sollte dann 39 x 2 (Datenpunkte x Hauptkomponenten) Dimensionen haben.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7] hier Code eingeben\n"
   ]
  },
  {
   "source": [
    "Diese finale, transformierte Matrix können wir nun mit Hilfe von `pandas` in einem DataFrame umwandeln, das erleicntert das weitere Arbeiten damit. Gleichzeitig ergänzen wir in dem DataFrame eine Spalte mit dem Parameter \"Flaechennutzung\". \n",
    "\n",
    "Ergänzt nun in dem Skript unten mit Hilfe von `matplotlib` einen Scatterplot der Daten mit den beiden Hauptkomponenten als Achsen, und der Flaechennutzung als Farbe der Punkte. Was lässt sich anhand dieser Abbildung über die Parameterwerte in Bezug auf die Flaechennutzung sagen?   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [8] aus ndarray einen DataFrame erzeugen \n",
    "principalDf = pd.DataFrame(data = Y , columns = ['principal component 1', 'principal component 2'])\n",
    "# eine dritte Spalte mit den Werten der Flächennutzung ergänzen\n",
    "finalDf = pd.concat([principalDf,pd.DataFrame(data,columns = ['Flaechennutzung'])], axis = 1) \n",
    "\n",
    "# hier Abbildung erzeugen\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Hauptkomponentenanalyse mit sklearn\n",
    "\n",
    "Das Python Package `sklearn` enthält viele nützliche Funtkionen für statistische Analysen und maschinelles Lernen. Darunter auch eine Funktion für Hauptkomponentenanalyse `sklearn.decomposition.PCA()`. Definiert dafür zuerst ein Objekt mit der genauen Methode (z.B. als \"model\") mit der Funktion `sklearn.decomposition.PCA()`. Definiert als Input wie viele Hauptkomponenten (\"n_components=2\") Ihr ausgegeben haben möchtet. \n",
    "\n",
    "Dann könnt Ihr den reduzierten Datensatz berechnen, indem Ihr auf diese Methode das Attribut `.fit_transform()` anwendet, mit dem ursprünglichen Datensatz als Input. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9] hier Code eingeben"
   ]
  },
  {
   "source": [
    "Für die Bewertung der Aussagekraft einer Hauptkomponentenanalyse ist es wichtig zu wissen, wie viel der ursprünglich Varianz (und damit der Informationen) in dem neuen transformierten Datensatz enthalten ist. Für die einzelnen Komponenten könnt Ihr das ausrechnen, indem Ihr das Attribut `.explained_variance_ratio_` auf Euer PCA Objekt anwendet. Berechnet außerdem die Summe der Varianzen. \n",
    "\n",
    "Wie würdet Ihr die Werte einordnen und die Aussagekraft bewerten?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10] hier Code eingeben"
   ]
  },
  {
   "source": [
    "Nun stellt auch die Ergebnisse der PCA mit `sklearn` analog zu oben graphisch dar, und vergleicht die beiden Ergebnisse."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [11] hier Code eingeben\n",
    "principalDf2 = pd.DataFrame(data = data_reduced, columns = ['principal component 1', 'principal component 2'])\n",
    "finalDf2 = pd.concat([principalDf2,pd.DataFrame(data,columns = ['Flaechennutzung'])], axis = 1) \n",
    "\n",
    "# hier Abbildung erzeugen"
   ]
  },
  {
   "source": [
    "Wenn Ihr alles richtig gemacht habt, sollten die beiden Abbildung das gleiche Bild zeigen (u.U. gespiegelt oder rotiert, also mit einem Vorzeichenwechsel auf einer oder beiden Achsen). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Clusteranalyse mit sklearn.KMeans\n",
    "\n",
    "Führt nun zum Abschluss noch eine Clusteranalyse mit dem k-Means Algorithmus und `sklearn.cluster.KMeans()` durch. Definiert dafür (analog zur PCA oben) zuerst wieder ein Objekt mit der Methode für 2 Cluster (\"n_clusters=2\"), und dann passt Eure Daten daran an (`model.fit(data)`). Die Zuordnung Eurer Daten in die Cluster könnt Ihr Euch über `model.predict(data)` als Vektor anzeigen lassen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [12] hier Code eingeben"
   ]
  },
  {
   "source": [
    "Fügt nun diesen Vektor mit der Clusterzuordnung zu Eurem ursprünglichen Datensatz hinzu, in dem Ihr ihn zuerst in einem pandas DataFrame umwandelt und dann mit den Datensatz zusammenführt (analog zu oben). Visualiert anschließend  die Ergebnisse in einem Scatterplot (über zwei beliebige Parameter des Datensatzes), und färbt die Punkte anhand der Clusterzuordnung ein. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [13] hier Code eingeben\n",
    "# neuen pandas DataFrame erzeugen\n",
    "cluster = pd.DataFrame(data = yhat, columns =['cluster'])\n",
    "# Daten und Cluster-Vektor zusammenbringen\n",
    "data_clus = pd.concat([data, cluster], axis = 1)\n",
    "\n",
    "# hier Abbildung erzeugen"
   ]
  },
  {
   "source": [
    "Vergleicht abschließend noch die Ergebnisse der PCA und Clusteranalyse. Was fällt dabei auf?\n",
    "\n",
    "## Ende\n",
    "\n",
    "### Referenzen: \n",
    "\n",
    "Koch et al. (2020), Groundwater fauna in an urban area: natural or affected? https://hess.copernicus.org/preprints/hess-2020-151/hess-2020-151.pdf\n",
    "\n",
    "Lever et al. (2017) Principal component analysis, Nature Methods 14(7), 641-642\n",
    "\n",
    "https://towardsdatascience.com/a-complete-guide-to-principal-component-analysis-pca-in-machine-learning-664f34fc3e5a\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}